{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/C241-PS517-ISALAT/Isalat-Model/blob/model_debug/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSyMHuCVys-O"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Dense,Conv2D,Flatten,MaxPooling2D,GlobalAveragePooling2D\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy as sp\n",
        "import cv2\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcRtMYQiEuj4",
        "outputId": "3b2ce8d9-75fe-4a8d-cd7c-d0f6391bde72"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = tf.keras.preprocessing.image_dataset_from_directory('/content/drive/My Drive/bisindo/data/train')\n",
        "validation_data = tf.keras.preprocessing.image_dataset_from_directory('/content/drive/My Drive/bisindo/data/validation')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygIrW44JEz3M",
        "outputId": "887fb48b-61e6-488a-adae-e5ee54f8d6a1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 234 files belonging to 26 classes.\n",
            "Found 78 files belonging to 26 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_data.take(1):\n",
        "  print(images.shape)  # Should print: (batch_size, 300, 300, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okIr6sRwMoY3",
        "outputId": "114d7ed5-56c1-4902-8b36-9973a08b93de"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 300, 300, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyCoMd93zpc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d29be806-aed4-4780-8a82-2752b9ac9246"
      },
      "source": [
        "#build a model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(16,input_shape=(300,300,3),kernel_size=(3,3),activation='relu',padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(32,kernel_size=(3,3),activation='relu',padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(64,kernel_size=(3,3),activation='relu',padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(128,kernel_size=(3,3),activation='relu',padding='same'))\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_8 (Conv2D)           (None, 300, 300, 16)      448       \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPoolin  (None, 150, 150, 16)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 150, 150, 32)      4640      \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPoolin  (None, 75, 75, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 75, 75, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPoolin  (None, 37, 37, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 37, 37, 128)       73856     \n",
            "                                                                 \n",
            " global_average_pooling2d_2  (None, 128)               0         \n",
            "  (GlobalAveragePooling2D)                                       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 97569 (381.13 KB)\n",
            "Trainable params: 97569 (381.13 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEqbqodQqeBV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce33a2f8-73c9-40ce-8089-b474517201e4"
      },
      "source": [
        "#model train\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=tf.keras.optimizers.RMSprop(lr=0.001))\n",
        "model.fit(train_data,epochs=25)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 5s 69ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 2/25\n",
            "8/8 [==============================] - 4s 61ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 3/25\n",
            "8/8 [==============================] - 4s 64ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 4/25\n",
            "8/8 [==============================] - 4s 68ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 5/25\n",
            "8/8 [==============================] - 5s 58ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 6/25\n",
            "8/8 [==============================] - 4s 59ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 7/25\n",
            "8/8 [==============================] - 4s 61ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 8/25\n",
            "8/8 [==============================] - 4s 64ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 9/25\n",
            "8/8 [==============================] - 4s 60ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 10/25\n",
            "8/8 [==============================] - 4s 64ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 11/25\n",
            "8/8 [==============================] - 3s 69ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 12/25\n",
            "8/8 [==============================] - 4s 62ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 13/25\n",
            "8/8 [==============================] - 4s 66ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 14/25\n",
            "8/8 [==============================] - 3s 63ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 15/25\n",
            "8/8 [==============================] - 5s 60ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 16/25\n",
            "8/8 [==============================] - 4s 62ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 17/25\n",
            "8/8 [==============================] - 3s 60ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 18/25\n",
            "8/8 [==============================] - 4s 63ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 19/25\n",
            "8/8 [==============================] - 4s 61ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 20/25\n",
            "8/8 [==============================] - 4s 61ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 21/25\n",
            "8/8 [==============================] - 3s 64ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 22/25\n",
            "8/8 [==============================] - 5s 65ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 23/25\n",
            "8/8 [==============================] - 3s 64ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 24/25\n",
            "8/8 [==============================] - 3s 59ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n",
            "Epoch 25/25\n",
            "8/8 [==============================] - 4s 71ms/step - loss: 0.0000e+00 - accuracy: 0.0385\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fec328ad8a0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJGvrHIu0Vnt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad7c7d9f-9dc6-458e-cdf2-fe279cc62f8f"
      },
      "source": [
        "gap_weights = model.layers[-1].get_weights()[0]\n",
        "gap_weights.shape\n",
        "\n",
        "cam_model  = Model(inputs=model.input,outputs=(model.layers[-3].output,model.layers[-1].output))\n",
        "cam_model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_8_input (InputLayer  [(None, 300, 300, 3)]     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 300, 300, 16)      448       \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPoolin  (None, 150, 150, 16)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 150, 150, 32)      4640      \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPoolin  (None, 75, 75, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 75, 75, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPoolin  (None, 37, 37, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 37, 37, 128)       73856     \n",
            "                                                                 \n",
            " global_average_pooling2d_2  (None, 128)               0         \n",
            "  (GlobalAveragePooling2D)                                       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 97569 (381.13 KB)\n",
            "Trainable params: 97569 (381.13 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcyzeiaO0pQs"
      },
      "source": [
        "def show_cam(image_value, features, results):\n",
        "  '''\n",
        "  Displays the class activation map of an image\n",
        "\n",
        "  Args:\n",
        "    image_value (tensor) -- preprocessed input image with size 300 x 300\n",
        "    features (array) -- features of the image, shape (1, 37, 37, 128)\n",
        "    results (array) -- output of the sigmoid layer\n",
        "  '''\n",
        "\n",
        "  # there is only one image in the batch so we index at `0`\n",
        "  features_for_img = features[0]\n",
        "  prediction = results[0]\n",
        "\n",
        "  # there is only one unit in the output so we get the weights connected to it\n",
        "  class_activation_weights = gap_weights[:,0]\n",
        "\n",
        "  # upsample to the image size\n",
        "  class_activation_features = sp.ndimage.zoom(features_for_img, (300/37, 300/37, 1), order=2)\n",
        "\n",
        "  # compute the intensity of each feature in the CAM\n",
        "  cam_output  = np.dot(class_activation_features,class_activation_weights)\n",
        "\n",
        "  # visualize the results\n",
        "  print(f'sigmoid output: {results}')\n",
        "  print(f\"prediction: {'dog' if round(results[0][0]) else 'cat'}\")\n",
        "  plt.figure(figsize=(8,8))\n",
        "  plt.imshow(cam_output, cmap='jet', alpha=0.5)\n",
        "  plt.imshow(tf.squeeze(image_value), alpha=0.5)\n",
        "  plt.show()\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function to preprocess an image and show the CAM\n",
        "def convert_and_classify(image):\n",
        "\n",
        "  # load the image\n",
        "  img = cv2.imread(image)\n",
        "\n",
        "  # preprocess the image before feeding it to the model\n",
        "  img = cv2.resize(img, (300,300)) / 255.0\n",
        "\n",
        "  # add a batch dimension because the model expects it\n",
        "  tensor_image = np.expand_dims(img, axis=0)\n",
        "\n",
        "  # get the features and prediction\n",
        "  features,results = cam_model.predict(tensor_image)\n",
        "\n",
        "  # generate the CAM\n",
        "  show_cam(tensor_image, features, results)"
      ],
      "metadata": {
        "id": "qov44MBgUPv0"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image in validation_data.take(1):\n",
        "  convert_and_classify(image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "xfI7uMtaUSCi",
        "outputId": "0e3a597a-49a6-476b-d831-f5de2492c22b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Can't convert object to 'str' for 'filename'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-1d32c7eec659>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mconvert_and_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-f63f4587d62f>\u001b[0m in \u001b[0;36mconvert_and_classify\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# load the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# preprocess the image before feeding it to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Can't convert object to 'str' for 'filename'"
          ]
        }
      ]
    }
  ]
}